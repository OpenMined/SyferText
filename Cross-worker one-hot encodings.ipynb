{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "j1TJzItmgfbr"
   },
   "source": [
    "## 0. Installing Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "db903MBJiHKk",
    "outputId": "891c7062-14a6-45e0-df3b-c63269cacbd0"
   },
   "outputs": [],
   "source": [
    "# cd to SyferText/\n",
    "# And run the below commands\n",
    "# !conda activate openmined\n",
    "# !git checkout psi_one_hot\n",
    "# !pip uninstall syfertext -y\n",
    "# !python setup.py install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "S-32sdzn43AB",
    "outputId": "18488ca3-4c66-4d61-8603-ddc6a78c492f"
   },
   "outputs": [],
   "source": [
    "# # Install Git LFS\n",
    "# !curl -s https://packagecloud.io/install/repositories/github/git-lfs/script.deb.sh | sudo bash\n",
    "# !sudo apt-get install git-lfs\n",
    "# !git lfs install\n",
    "\n",
    "# # Install syfertext language model\n",
    "# ! pip install git+git://github.com/Nilanshrajput/syfertext_en_core_web_lg@master"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oIRRus3kkm72"
   },
   "source": [
    "## Vocabulary in Private NLP\n",
    "\n",
    "During training Language Models we often need to know the size of our vocabulary and represent the words in our vocabulary as one-hot-vectors. This is a trivial task while dealing with non-private and non-remote datasets. \n",
    "\n",
    "But things start to get complicated as we begin to deal with private datasets, cause the contents of the datasets are private we can't search for the unique words as before, thus making it a challenging task to represent the words as one-hot vectors.\n",
    "\n",
    "Fret Not ! SyferText to the rescue !! ;)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "id": "PwgR4Opwh6o6",
    "outputId": "8d0e6927-49c8-4ac4-bd13-38d00523352f",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Torch was already hooked... skipping hooking process\n"
     ]
    }
   ],
   "source": [
    "import syfertext\n",
    "import syft as sy\n",
    "import sys\n",
    "import torch\n",
    "import random\n",
    "\n",
    "from syft.generic.string import String\n",
    "\n",
    "from syfertext.workers.virtual import VirtualWorker\n",
    "from syfertext.encdec import encrypt, decrypt\n",
    "\n",
    "hook = sy.TorchHook(torch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WGIpWJM0h6pu",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# Remote workers\n",
    "me = sy.local_worker\n",
    "# me = syfertext.local_worker\n",
    "# me = VirtualWorker(hook, \"me\")\n",
    "\n",
    "# Language Object\n",
    "nlp = syfertext.load(\"en_core_web_lg\", owner=me)\n",
    "\n",
    "bob = VirtualWorker(hook, \"bob\")\n",
    "alice = VirtualWorker(hook, \"alice\")\n",
    "carol = VirtualWorker(hook, \"carol\")\n",
    "david = VirtualWorker(hook, \"david\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Jd13unNXh6qS",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# Simulate private datasets\n",
    "workers = [bob, alice, carol, david]\n",
    "\n",
    "texts = [\"I am learning nlp during lockdown.\",\n",
    "         \"I am learning cryptography during quarantine.\",\n",
    "         \"I am learning more about privacy.\",\n",
    "         \"I am building syfertext during quarantine.\",\n",
    "     ]\n",
    "\n",
    "# Send text to workers\n",
    "string_pointers = list()\n",
    "for text, worker in zip(texts, workers):\n",
    "    str_ptr = String(text).send(worker)\n",
    "    string_pointers.append(str_ptr)\n",
    "\n",
    "# Tokenize the strings and get doc pointers\n",
    "docs = list()\n",
    "for str_ptr in string_pointers:\n",
    "    doc = nlp(str_ptr)\n",
    "    docs.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jsRL_7pZXAXY",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# Store the docs in corresponding variables\n",
    "# Will be used later\n",
    "bob_doc = docs[0]\n",
    "alice_doc = docs[1]\n",
    "carol_doc = docs[2]\n",
    "david_doc = docs[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "usm-5Mj2qS5o"
   },
   "source": [
    "## 1. Private NLP Setup\n",
    "\n",
    "While dealing with private datasets, we can't bring them to the local machine in their raw format. So instead we can first encrypt them on the remote machines and then bring them to the local machine for comparison. But, there are a few requirements for the encryption scheme that we should follow:\n",
    "\n",
    "1. **Symmetric Key Encryption**: The keys used by each of the workers to encrypt their tokens should be same, cause only then the similar tokens reisdig on different workers can be compared to each other in their encrypted state.\n",
    "\n",
    "2. **Determinstic Encryption**: The workers should encrypt their tokens using deterministic encryption, cause adding randomness to the encryption will not allow us to compare similar tokens even when they are encrypted with the same keys.\n",
    "\n",
    "Hence, to fullfill these conditions we will be using the Diffie-Hellman Key exchange process and AES (Advance Encryption Scheme) in the ECB (Deterministic) Mode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RWV_Sa9er16R"
   },
   "source": [
    "The PySyft BaseWorker has been extended with the following methods to allow us to perform the key exchange protocol easily.\n",
    "\n",
    "\n",
    "```\n",
    "_generate_private_key()\n",
    "generate_public_key()\n",
    "generate_secret_key()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8As64pMBsvvY",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "shared_prime = 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n",
    "shared_base = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "elvHRa1Ar1aM",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# Generate private keys\n",
    "key_len = random.randint(0, 128) % 64\n",
    "bob.generate_private_key(key_len)\n",
    "\n",
    "key_len = random.randint(0, 128) % 64\n",
    "alice.generate_private_key(key_len)\n",
    "\n",
    "# Generate public keys\n",
    "bob_public_key = bob.generate_public_key(shared_prime, shared_base)\n",
    "alice_public_key = alice.generate_public_key(shared_prime, shared_base)\n",
    "\n",
    "# Generate secret keys\n",
    "alice.generate_secret_key(shared_prime, bob_public_key)\n",
    "bob.generate_secret_key(shared_prime, alice_public_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "49QpCRSks3gL"
   },
   "source": [
    "Now Alice and Bob both have a **secret key**, which is same for both of them and known only to them. This enables us to get the encrypted tokens and find the distinct tokens among them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "id": "WVJ4g_sutFWH",
    "outputId": "9de1224d-b90f-4773-d9ad-4a746b082a6b",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bob's encrypted tokens: {b'10YvMHBVBFOecTtdhzWJJg==', b'Zx4vbNJ54aF8gkqNp3WBBA==', b'j/zIpbnjMag5438PC4vbgA==', b'XGk6Xzddijf4OYZj3nB/CQ==', b'GSIe1b+JgiXoVfY2gmBqeQ==', b'V0jknt6pDFJ5Tuzx3aiFQg==', b'8poeuCWT2bSkf9SP03Ov2A=='}\n",
      "--------------------\n",
      "Alice's encrypted tokens: {b'10YvMHBVBFOecTtdhzWJJg==', b'Zx4vbNJ54aF8gkqNp3WBBA==', b'mnX3g6mQL2w/+1D9VcWhkA==', b'gqS+mHl6dB0krvOj6SCHlA==', b'XGk6Xzddijf4OYZj3nB/CQ==', b'V0jknt6pDFJ5Tuzx3aiFQg==', b'8poeuCWT2bSkf9SP03Ov2A=='}\n"
     ]
    }
   ],
   "source": [
    "bob_enc_tokens = bob_doc.get_encrypted_tokens_set()\n",
    "alice_enc_tokens = alice_doc.get_encrypted_tokens_set()\n",
    "\n",
    "# Print a sample of encrypted tokens\n",
    "print(\"Bob's encrypted tokens:\", bob_enc_tokens)\n",
    "print(\"-\"*20)\n",
    "print(\"Alice's encrypted tokens:\", alice_enc_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "v1TJS76ut0SW"
   },
   "source": [
    "These sets can be compared to each other and the unique tokens can be determined from them.\n",
    "Let's write some handy functions to do it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "S9s-yv48mv4C",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def assign_indices(all_tokens):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        all_tokens (Set): Set consisting\n",
    "            tokens across all workers\n",
    "    Returns:\n",
    "        token_to_index (dict): Maps all unique \n",
    "            tokens to unique indices\n",
    "    \"\"\"\n",
    "    all_tokens = list(all_tokens)\n",
    "    \n",
    "    # Shuffle the tokens to avoid any information\n",
    "    # leakage due to the order of indices\n",
    "    random.shuffle(all_tokens)\n",
    "    \n",
    "    indices = [i for i in range(len(all_tokens))]\n",
    "    token_to_index = dict(zip(all_tokens, indices))    # Map tokens to indices\n",
    "    return token_to_index\n",
    "\n",
    "def map_to_indices(workers_tokens, token_to_index):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        workers_tokens (Iterable): Consists of tokens\n",
    "            belonging to a specific worker.\n",
    "        token_to_index (dict): Map of all unique tokens \n",
    "            to unique indices\n",
    "    \n",
    "    Returns:\n",
    "        worker_token_to_index (dict): Each token in\n",
    "            `worker_tokens` is mapped to an index\n",
    "    \"\"\"\n",
    "    worker_token_to_index = {}    \n",
    "    for token in workers_tokens:\n",
    "        # map token to index\n",
    "        worker_token_to_index[token] = token_to_index[token]    \n",
    "    return worker_token_to_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size:  9 \n",
      "\n",
      "\n",
      "Encrypted Token:  b'10YvMHBVBFOecTtdhzWJJg==' \t\tIndex:  0\n",
      "Encrypted Token:  b'GSIe1b+JgiXoVfY2gmBqeQ==' \t\tIndex:  1\n",
      "Encrypted Token:  b'j/zIpbnjMag5438PC4vbgA==' \t\tIndex:  2\n",
      "Encrypted Token:  b'Zx4vbNJ54aF8gkqNp3WBBA==' \t\tIndex:  3\n",
      "Encrypted Token:  b'V0jknt6pDFJ5Tuzx3aiFQg==' \t\tIndex:  4\n",
      "Encrypted Token:  b'8poeuCWT2bSkf9SP03Ov2A==' \t\tIndex:  5\n",
      "Encrypted Token:  b'XGk6Xzddijf4OYZj3nB/CQ==' \t\tIndex:  6\n",
      "Encrypted Token:  b'mnX3g6mQL2w/+1D9VcWhkA==' \t\tIndex:  7\n",
      "Encrypted Token:  b'gqS+mHl6dB0krvOj6SCHlA==' \t\tIndex:  8\n"
     ]
    }
   ],
   "source": [
    "def print_dict(token_to_index):\n",
    "    for enc_token, index in token_to_index.items():\n",
    "        print(\"Encrypted Token: \", enc_token, \"\\t\\tIndex: \", index)\n",
    "\n",
    "\n",
    "all_tokens = bob_enc_tokens.union(alice_enc_tokens)  # Take the union of both sets\n",
    "VOCAB_SIZE = len(all_tokens)                         # store the vocaulary size\n",
    "\n",
    "token_to_indices = assign_indices(all_tokens)\n",
    "\n",
    "print(\"Vocabulary size: \", VOCAB_SIZE, \"\\n\\n\")\n",
    "print_dict(token_to_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bob's Tokens\n",
      "\n",
      "Encrypted Token:  b'10YvMHBVBFOecTtdhzWJJg==' \t\tIndex:  0\n",
      "Encrypted Token:  b'Zx4vbNJ54aF8gkqNp3WBBA==' \t\tIndex:  3\n",
      "Encrypted Token:  b'j/zIpbnjMag5438PC4vbgA==' \t\tIndex:  2\n",
      "Encrypted Token:  b'XGk6Xzddijf4OYZj3nB/CQ==' \t\tIndex:  6\n",
      "Encrypted Token:  b'GSIe1b+JgiXoVfY2gmBqeQ==' \t\tIndex:  1\n",
      "Encrypted Token:  b'V0jknt6pDFJ5Tuzx3aiFQg==' \t\tIndex:  4\n",
      "Encrypted Token:  b'8poeuCWT2bSkf9SP03Ov2A==' \t\tIndex:  5\n",
      "\n",
      "Alice's Tokens\n",
      "\n",
      "Encrypted Token:  b'10YvMHBVBFOecTtdhzWJJg==' \t\tIndex:  0\n",
      "Encrypted Token:  b'Zx4vbNJ54aF8gkqNp3WBBA==' \t\tIndex:  3\n",
      "Encrypted Token:  b'mnX3g6mQL2w/+1D9VcWhkA==' \t\tIndex:  7\n",
      "Encrypted Token:  b'gqS+mHl6dB0krvOj6SCHlA==' \t\tIndex:  8\n",
      "Encrypted Token:  b'XGk6Xzddijf4OYZj3nB/CQ==' \t\tIndex:  6\n",
      "Encrypted Token:  b'V0jknt6pDFJ5Tuzx3aiFQg==' \t\tIndex:  4\n",
      "Encrypted Token:  b'8poeuCWT2bSkf9SP03Ov2A==' \t\tIndex:  5\n"
     ]
    }
   ],
   "source": [
    "# Assign indices to bob's tokens\n",
    "bob_token_to_index = map_to_indices(bob_enc_tokens, token_to_indices)\n",
    "\n",
    "# Assign indices to alice's tokens\n",
    "alice_token_to_index = map_to_indices(alice_enc_tokens, token_to_indices)\n",
    "\n",
    "print(\"Bob's Tokens\\n\")\n",
    "print_dict(bob_token_to_index)\n",
    "\n",
    "print(\"\\nAlice's Tokens\\n\")\n",
    "print_dict(alice_token_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's now return the token_to_index to the respective docs\n",
    "# So that later on we can query them to get index for the tokens\n",
    "# they contain.\n",
    "bob_doc.set_indices(bob_token_to_index)\n",
    "alice_doc.set_indices(alice_token_to_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we are able to find all the unique words across all workers without leaking any information about:\n",
    "1. Tokens that the Bob and Alice contain to the local worker\n",
    "2. Tokens that Bob contains to Alice and vice-versa\n",
    "\n",
    "But, if there is a huge limitation to this approach. Let's see what it is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Uw9ifYoUh6qz"
   },
   "source": [
    "## 2. Man-In-The-Middle Attack\n",
    "\n",
    "One of the major drawbacks with the **public keys passing through the local worker is that the local worker can perform the man-in-the-middle attack**, which is a well-known limitation of the diffie-hellman key exchange. This allows the local worker to decrypt both Alice and Bob's tokens.\n",
    "\n",
    "Note: `The key concept of this attack is that the local worker impersonates itself as a data provider. And it establishes trus with both Bob and Alice thus enabling it to decrypt both of their tokens.`.\n",
    "\n",
    "Let's demonstrate how this can be done. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ueZporjwZiKr"
   },
   "outputs": [],
   "source": [
    "# Local worker capable of performing DH key exchange\n",
    "local_worker = VirtualWorker(hook, \"local_worker\")\n",
    "\n",
    "# Generate private key for local worker\n",
    "key_len = random.randint(0, 128) % 64\n",
    "local_worker.generate_private_key(key_len)\n",
    "\n",
    "# Generate public key for local worker\n",
    "local_public_key = local_worker.generate_public_key(shared_prime, shared_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LF_U2zG6h6rB"
   },
   "outputs": [],
   "source": [
    "# Generate fresh public keys for bob and alice\n",
    "bob_public_key = bob.generate_public_key(shared_prime, shared_base)\n",
    "alice_public_key = alice.generate_public_key(shared_prime, shared_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 89
    },
    "colab_type": "code",
    "id": "8l60ZHAMh_3l",
    "outputId": "a42483a1-495f-4c30-9f1a-0f6b1351c5e0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bob's compromised secret key   : \n",
      " b',\\xc4\\xe1\\x9d\\x14o\\x037\\x1f\\xf5\\x08E\\xd3\\xee\\xde.\\x12\\xc1\\xc5\\xae\\x83G\\x93\\xcc\\x9b\\xdf\\xd2\\x01C\\xb7v\\x00' \n",
      "\n",
      "Alice's compromised secret key : \n",
      " b'\\x9f\\x8dG\\xb2\\x9e\\xf9Q\\xe0%yc\\xb67acBOV\\x0c\\x8f\\x91\\x8e\\xdca\\xe79I\\xc5N\\\\\\xe3\\x99'\n"
     ]
    }
   ],
   "source": [
    "\"\"\" This is the Key Step. \"\"\"\n",
    "# Now instead of sending bob's public key to alice and alice's \n",
    "# public key to bob, the local worker can pass both of them it's\n",
    "# own public key.\n",
    "alice.generate_secret_key(shared_prime, local_public_key)\n",
    "bob.generate_secret_key(shared_prime, local_public_key)\n",
    "\n",
    "# And using bob's public key, local worker\n",
    "# can generate a secret key which is similar to the one\n",
    "# with bob.\n",
    "local_worker.generate_secret_key(shared_prime, bob_public_key)\n",
    "bob_compromised_key = local_worker.secret_key\n",
    "\n",
    "# And using alice's public key, local worker\n",
    "# can generate a secret key which is similar to the one\n",
    "# with alice.\n",
    "local_worker.generate_secret_key(shared_prime, alice_public_key)\n",
    "alice_compromised_key = local_worker.secret_key\n",
    "\n",
    "print(\"Bob's compromised secret key   : \\n\", bob_compromised_key, '\\n')\n",
    "print(\"Alice's compromised secret key : \\n\", alice_compromised_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fXMaj5BCbJ-B"
   },
   "source": [
    "Now let us try to bob's and alice's tokens using their compromised secret keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "id": "tgBZlPNUjfvl",
    "outputId": "7fe23fbc-eb10-4b42-d4c3-0b7d48b14130"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bob's data: {b'0B7YpBgrKjM2WGpkl12O+A==', b'3LcjNspRlarwehR9tUui8Q==', b'9C8qeM4SmAaCYJ2I8R/i9w==', b'sXv3DNDX8zpBB9Dgd4DxMA==', b'NmueVuCywxR2xFF8vH67XQ==', b'wFDm2nG3GaU9UVh4mPibKw==', b'Wd+1O+CFV9qKCHw3b/aLFA=='}\n",
      "----------\n",
      "Alice's data: {b'nqwiXu7aQGOKdYVNqOU0Eg==', b'qDIlpOXHnlJHo+rlWMyIbg==', b'ecc44PQn47cZNmRzt5NYuw==', b'L/h44wQ/zSSZiCI4eTJMsA==', b'JRrd4g7P3kvZtTWN+CtFng==', b'MqCnbKCiGDQg1fmqZ5U2Ow==', b'+35fRy0xRRo3Pb0M8+GzUg=='}\n"
     ]
    }
   ],
   "source": [
    "# First we will get the encrypted tokens\n",
    "bob_enc_tokens = bob_doc.get_encrypted_tokens_set()\n",
    "alice_enc_tokens = alice_doc.get_encrypted_tokens_set()\n",
    "\n",
    "# Print the encrypted tokens tokens\n",
    "print(\"Bob's data:\", bob_enc_tokens)\n",
    "print('-'*10)\n",
    "print(\"Alice's data:\", alice_enc_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 454
    },
    "colab_type": "code",
    "id": "6UoYv8Xdh6r8",
    "outputId": "febb8e10-2f6d-4834-a0b1-7482f0bbca85"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bob's private data: ['nlp', 'learning', 'am', 'I', 'during', 'lockdown', '.']\n",
      "----------\n",
      "Alice's private data: ['learning', 'during', 'cryptography', 'quarantine', 'I', '.', 'am']\n"
     ]
    }
   ],
   "source": [
    "def decrypt_tokens(enc_tokens, secret_key):\n",
    "    \"\"\" Decrypts a set of enc_tokens using\n",
    "    passsed in secret_key. \"\"\"\n",
    "    dec_tokens = list()\n",
    "    for token in enc_tokens:\n",
    "        dec_token = decrypt(token, secret_key).decode(\"utf-8\")\n",
    "        dec_tokens.append(dec_token)\n",
    "    return dec_tokens\n",
    "\n",
    "bob_dec_tokens = decrypt_tokens(bob_enc_tokens, \n",
    "                                bob_compromised_key)\n",
    "\n",
    "alice_dec_tokens = decrypt_tokens(alice_enc_tokens, \n",
    "                                  alice_compromised_key)\n",
    "\n",
    "# Print the decrypted tokens\n",
    "print(\"Bob's private data:\", bob_dec_tokens)\n",
    "print('-'*10)\n",
    "print(\"Alice's private data:\", alice_dec_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In just a few extra steps the local worker is able to decrypt both Bob's and Alice's encrypted tokens. So we must find a solution to avoid this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XGn36gtch6sY"
   },
   "source": [
    "## 3. DHKE on SecureWorker\n",
    "\n",
    "DHKE: Diffie-Hellman Key Exchange\n",
    "\n",
    "In general, the local worker is the one who is training a model.\n",
    "For eg, it can be a company developing a health product and training a Deep Learning model on the data residing on the hospitals servers.\n",
    "But as mentioned above, if the local worker is malicious, it is capable of decrypting the data present on the hospital's machines.\n",
    "\n",
    "Thus, in order to avoid the data-breach and protect the privacy of the data belonging to the workers, the **workers can decide to trust a third-party worker** who acts as a neutral party. This worker called as the SecurWorker is trustworthy and **performs the Diffie-Hellman Key Exchange** between all workers.\n",
    "\n",
    "So, in our hospital example, the SecureWorker can be an **AWS instance**. Moreover, it is required only for a very short instance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qjZdvAf3euO-"
   },
   "source": [
    "The SecureWorker is capable of performing a secure key-exchange process between two or more workers. Here we will present an example, with **four workers**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iyhGbvJTh6sc"
   },
   "outputs": [],
   "source": [
    "workers = [bob, alice, carol, david]\n",
    "secure_worker = VirtualWorker(hook, id=\"james\")\n",
    "\n",
    "# Execute the DH key exchange protocol securely on Secure Worker\n",
    "secure_worker.execute_dh_key_exchange(shared_prime, shared_base, workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jXodTFOwfJvy"
   },
   "source": [
    "##### And that's it!! ;)\n",
    "\n",
    "Now all the workers have a secret key on their machines, which is same for all of them, and thus enables us the local-worker to perform the **Set Intersection**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hQxjHW3Bh6sr",
    "outputId": "452e706f-8984-4e3b-bff9-e513cbcbf84c"
   },
   "outputs": [],
   "source": [
    "# List of Sets of encryted tokens\n",
    "enc_token_sets = list()\n",
    "\n",
    "for doc in docs:\n",
    "    enc_tokens = doc.get_encrypted_tokens_set()\n",
    "    enc_token_sets.append(enc_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PQNp_qNoh6s2"
   },
   "outputs": [],
   "source": [
    "all_tokens = set()\n",
    "\n",
    "for token_set in enc_token_sets:\n",
    "    # Union with the current set\n",
    "    all_tokens = all_tokens.union(token_set)\n",
    "\n",
    "token_to_index = assign_indices(all_tokens)\n",
    "\n",
    "# Map each doc's tokens to indices\n",
    "for doc, enc_tokens in zip(docs, enc_token_sets):\n",
    "\n",
    "    cur_token_to_index = map_to_indices(enc_tokens, token_to_index)\n",
    "    \n",
    "    # Return the mapped tokens to doc\n",
    "    doc.set_indices(cur_token_to_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "i_Q6qSSihDhI"
   },
   "source": [
    "## 4. One-Hot Vectors\n",
    "\n",
    "Now let's learn how to get one-hot vectors from documents present on multiple workers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(all_tokens)  # vocabulary size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MhQHbY_gh6th"
   },
   "source": [
    "## 5. Diffference between our approach and PSI\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extensive research work already present in the field of PSI can be referred, but our problem statement is slightly different from PSI\n",
    "- In PSI, the output (set intersection) is returned to one of the parties. In our case, it would be better if **we don't let any worker know the set intersection**. Thus avoiding leaking any info to the workers about the data present on the other workers. They just receive their tokens with indices assigned to each token. Nothing else."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "PSI_TUTORIAL.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3.6.10 64-bit ('openmined': conda)",
   "language": "python",
   "name": "python361064bitopenminedconda7d626f6923e74ad6af0109078ac21f5f"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
