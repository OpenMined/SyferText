{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started: Remote Tokenization\n",
    "----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the main motivations behind the creation of SyferText is to provide the ability to preprocess strings residing on a remote machine. Tokenization is one such important preprocessing step when we create deep learning NLP models. \n",
    "\n",
    "SyferText leverages [PySyft](https://github.com/OpenMined/PySyft)'s distributed architecture and its privacy-preserving arsenal of tools to enable 'blind' tokenization of private strings residing on remote workers without revealing their contents.\n",
    "\n",
    "In this tutorial, you will learn how to use SyferText to tokenize a PySyft `String` residing on a remote private worker.\n",
    "\n",
    "A private worker is one whose data is considered as sensitive with restricted access. We do not have the right to read this data. SyferText enables us to 'blindly' tokenize such a string and get an encrypted version of its tokens' vector embeddings. With these encrypted embeddings, one could train an encrypted deep learning model using PySyft. \n",
    "\n",
    "Training an NLP model with encrypted embeddings will be the subject of an upcoming tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Author\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `Alan Aboudib`  -> [@alan_aboudib](https://twitter.com/alan_aboudib) (Twitter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------\n",
    "\n",
    "## 1. `SyferText`'s distributed architecture\n",
    "\n",
    "-------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have seen in a [previous tutorial](https://bit.ly/2RQ9lwl) that SyferText can be used to tokenize a PySyft string on a local worker. In such setting, all of the processing components such as the `Language`, `Tokenizer`, `Doc` and `Token` objects were created on the same worker.\n",
    "\n",
    "What you haven't seen yet is that SyferText's `Language` object can also be used to tokenize a PySyft `StringPointer` object, which is a pointer to a PySyft `String` on a remote worker.\n",
    "\n",
    "When the `Language` object receives a `StringPointer` to tokenize, some changes to its architecture are applied; some of its components are sent to the remote worker where the string lives. Here is how this is done:\n",
    "\n",
    "1. The `Language` object receives a `StringPointer` pointing to a PySyft `String` to be tokenized.\n",
    "2. A `Tokenizer` is created and sent to the worker where the `String` is.\n",
    "3. A `TokenizerPointer` object pointing to the remote `Tokenizer` is created and kept at the local worker.\n",
    "4. The remote tokenizer tokenizes the string on the remote worker and creates a `Doc` containing all of the `Token` objects.\n",
    "5. A `DocPointer` object pointing at the remote `Doc` is kept at the local worker.\n",
    "\n",
    "Here is an illustration of the steps I mentioned above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![SyferText architecture: local case](art/syfertext_remote.png \"SyferText architecture on remote workers\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the `DocPointer` in the illustration above does not provide access to the individual tokens as a `Doc` object does. This is because the string on the remote worker is considered as private; we are not allowed to reveal its tokens or bring them to our local machine. We are only allowed to obtain encrypted versions of them.\n",
    "\n",
    "Let's now do some coding:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------\n",
    "\n",
    "## 2. Tokenizing a remote PySyft `String`\n",
    "\n",
    "-------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first import SyferText. Since SyferText is based on PySyft, we also need to import the latter, as well as PyTorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hide warnings (nothing to do with SyferText)\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Torch was already hooked... skipping hooking process\n",
      "WARNING:root:Torch was already hooked... skipping hooking process\n"
     ]
    }
   ],
   "source": [
    "import syft as sy\n",
    "import torch\n",
    "import syfertext\n",
    "\n",
    "# Import PySyft's String class\n",
    "from syft.generic.string import String"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now need to hook PyTorch using the TorchHook in PySyft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Torch was already hooked... skipping hooking process\n"
     ]
    }
   ],
   "source": [
    "hook = sy.TorchHook(torch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will endow PyTorch with magic powers, privacy-preserving deep learning powers, such as Federated Learning, Differential Privacy, Encrypted Training tools and more. To learn more about PySyft, you can check out its awesome [tutorial notebooks](https://github.com/OpenMined/PySyft/tree/master/examples/tutorials)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every machine in PySyft is called a worker. Since we are using SyferText to tokenize a string on a remote machine, then we should get an instance of the object representing that worker. So let's now create fours workers. We will call the local worker 'me', and the remote workers 'bob' and 'alice'. A fourth worker that will act as the crypto provider for SMPC, the encryption algorithm PySyft uses, should also be created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The local worker\n",
    "me = hook.local_worker\n",
    "\n",
    "# The remote workers\n",
    "bob = sy.VirtualWorker(hook, id = 'bob')\n",
    "alice = sy.VirtualWorker(hook, id = 'alice')\n",
    "\n",
    "# The crypto provider\n",
    "crypto_provider = sy.VirtualWorker(hook, id = 'crypto_provider')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to load the language model. The only language model available for the moment in SyferText is `en_core_web_lg`, which is a model for English language simplified from spaCy's language model with the same name. Check out the  properties of that model [here](https://spacy.io/models/en#en_core_web_lg)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(syfertext.language.Language, <VirtualWorker id:me #objects:0>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp = syfertext.load('en_core_web_lg', owner = me)\n",
    "\n",
    "type(nlp), nlp.owner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice from the cell's output that the `nlp` variable is an object of the `Language` class, and similar to all PySyft objects, it has an owner, which is a PySyft `VirtualWorker` representing our local machine. This corresponds to the illustration I showed you above were the `Language` object was shown to reside in the local worker."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now going to create a PySyft `String` and send it to `bob`. This will be our remote string that we are going to tokenize:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "syft.generic.pointers.string_pointer.StringPointer"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string = String('A string to tokenize')\n",
    "\n",
    "# Send the string to the remote worker `bob`\n",
    "string_ptr = string.send(bob)\n",
    "\n",
    "type(string_ptr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the variable `string_ptr` is a `StringPointer` object returned by the `send()` method of the worker object. Let's make sure that the string is really sent to bob."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{19852353146: 'A string to tokenize'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bob._objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that `bob`'s object store now includes the PySyft string we are willing to tokenizer.\n",
    "\n",
    "You might have noticed that printing out the `_objects` attribute has actually revealed the string text. This of course violates our assumption that the remote string is private. This is one main developement issue SyferText is going to integrate in the near future.\n",
    "\n",
    "We now have all what we need to tokenize the string. Let's do it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'syfertext.pointers.doc_pointer.DocPointer'>\n",
      "[DocPointer | me:28397382893 -> bob:69172448231]\n"
     ]
    }
   ],
   "source": [
    "doc_ptr = nlp(string_ptr)\n",
    "\n",
    "# Some checks\n",
    "print(type(doc_ptr))\n",
    "print(doc_ptr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above output, you can see that calling `nlp` with the `StringPointer` as its argument returns a `DocPointer` object that points to a `Doc` with a specified ID residing on `bob`, the remote worker.\n",
    "\n",
    "Let's check out if `bob` really has a `Doc` object with that ID:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{19852353146: 'A string to tokenize',\n",
       " 80082181733: SubPipeline[tokenizer],\n",
       " 69172448231: Doc>None}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bob._objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It does! `bob` has a `Doc` with the same ID specified by the `DocPointer` printout above. Moreover, the `Tokenizer` also resides on `bob` which is what you saw in the above illustration of SyferText's architecture at the beginning of this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using that `DocPointer` we can now get an SMPC-encrypted string vector. For the moment, the string vector here is simply a concatenation of the individual token vectors of the string. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Vector size is 300\n",
      "(Wrapper)>AutogradTensor>FixedPrecisionTensor>[AdditiveSharingTensor]\n",
      "\t-> [PointerTensor | me:9449974281 -> bob:77267367866]\n",
      "\t-> [PointerTensor | me:8819687575 -> alice:3303958623]\n",
      "\t*crypto provider: crypto_provider*\n"
     ]
    }
   ],
   "source": [
    "doc_vector_enc = doc_ptr.get_encrypted_vector(bob, alice, crypto_provider = crypto_provider)\n",
    "\n",
    "print(f' Vector size is {doc_vector_enc.shape[0]}')\n",
    "print(doc_vector_enc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the returned vector is an SMPC-encrypted PySyft `AdditiveSharingTensor` object. It is encrypted into two shares residing in `bob` and `alice` workers. It has a size of 300 which corresponds to the size of vector obtained by average of 4 token vectors each of size 300.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Vector size is torch.Size([4, 300])\n",
      "(Wrapper)>AutogradTensor>FixedPrecisionTensor>[AdditiveSharingTensor]\n",
      "\t-> [PointerTensor | me:70696031469 -> bob:14780350359]\n",
      "\t-> [PointerTensor | me:61920254224 -> alice:55516327905]\n",
      "\t*crypto provider: crypto_provider*\n"
     ]
    }
   ],
   "source": [
    "tok_vectors = doc_ptr.get_encrypted_token_vectors(bob, alice, crypto_provider = crypto_provider)\n",
    "print(f' Vector size is {tok_vectors.shape}')\n",
    "print(tok_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A SMPC-encrypted PySyft `AdditiveSharingTensor` representing the array of all vectors in the document this pointer points to.\n"
>>>>>>> master
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### That's it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You now know how to use SyferText to tokenize a remote string by manipulating a string pointer. You have seen how SyferText's distributed architecture make that easy by leveraging PySyft which, in turn, handles all the communication logic between workers.\n",
    "\n",
    "However, since SyferText is still in its developement phase, more features and methods are constantly being added and/or modified. We will make sure to update this tutorial whenever such new features are released."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have any questions or suggestions, you can DM me on OpenMined's [slack channel](http://slack.openmined.org/), or otherwise contact me directly on my [Twitter page](https://twitter.com/alan_aboudib)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
